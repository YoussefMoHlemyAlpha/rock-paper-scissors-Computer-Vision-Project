# rock-paper-scissors-Computer-Vision-Project using Yolo V8
This project focuses on creating a Rock-Paper-Scissors Detection System using the YOLOv8 object detection model. It involves training the model on a custom dataset and enabling real-time predictions through a webcam or video feed. The key steps include dataset preparation, model training, validation, and inference.

The dataset, sourced from Roboflow, contains labeled images of hand gestures representing "rock," "paper," and "scissors." The YOLOv8 model is trained for 50 epochs to accurately detect and classify these gestures.

Key features of the project:

Dataset Management: Utilized Roboflow for dataset preparation and augmentation.
YOLOv8 Framework: Employed for efficient object detection and training.
Real-Time Detection: Integrated webcam input to make real-time predictions of hand gestures.
This project demonstrates the effectiveness of YOLOv8 for gesture-based classification tasks and serves as a foundation for interactive applications such as gesture-controlled games or educational
